{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEOP dataset preprocessing 2018 release\n",
    "MEOP data can be found at: http://www.meop.net/database/format.html\n",
    "MEOP stands for: \"Marine Mammals Exploring the Oceans from Pole to Pole\".\n",
    "MEOP consists of ARGO like profiles, however, the instruments are fitted onto marine mammals.\n",
    "The UK dataset of MEOP contains many profiles in the Weddel Sea region, the following analysis is of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/km/anaconda2/envs/py35new/lib/python3.5/site-packages/xarray/conventions.py:9: FutureWarning: The pandas.tslib module is deprecated and will be removed in a future version.\n",
      "  from pandas.tslib import OutOfBoundsDatetime\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gsw\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import pdb\n",
    "from IPython.display import Image\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.io import netcdf\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from haversine import haversine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all the paths to the netcdf files are given in a column in the file named \"filenames.txt\"\n",
    "\n",
    "This can be done by \n",
    "1. navigating to the myScripts folder in the terminal and typing in for eg. \n",
    ": \"ls ../USA/DATA_ncARGO/ |grep .nc | xargs -i echo \"../USA/DATA_ncARGO/{}\" >> filenames.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is the creation of the merged csv file from the separate netcdf files. The Pandas package is able to efficiently read the csv file and then perform operations on the data.  Only the countries with southern hemisphere profiles are selected in the \"filenames.txt\" list.\n",
    "\n",
    "An alternative and perhaps more efficient way of processing the data would be to write it all in Xarrays. However, since I developed all the original codes and scripts for plotting etc. in pandas, I am sticking with pandas for **legacy** support reasons.\n",
    "\n",
    "The following operation is time consuming (should take 2 - 3 hrs at the most, but ensure at least 16GB RAM), but it is a one time operation. It requires a workstation with sufficient memory to read and write all the data. The CSV file has already been prepared and saved to disk, you can use that directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenamesMerged = pd.read_csv(\"filenames.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_df(filenames):\n",
    "    for i in range((len(filenames))):\n",
    "        arr_data = xr.open_dataset(filenames[0][i])\n",
    "        nlev = len(arr_data['PRES_ADJUSTED'][0])\n",
    "        nprof = len(arr_data['PRES_ADJUSTED'])\n",
    "        if(i == 0):\n",
    "            NPROF = np.arange(nprof)\n",
    "            NPROF = np.array([[NPROF[j]] * nlev for j in range(len(NPROF)) ])\n",
    "        else:\n",
    "            last_end = NPROF.flatten()[-1] + 1\n",
    "            NPROF = np.arange(last_end, last_end+nprof )\n",
    "            NPROF = np.array([[NPROF[j]] * nlev for j in range(len(NPROF)) ])\n",
    "        \n",
    "        ind = np.arange(nprof*nlev)\n",
    "        lat = np.array([[arr_data[\"LATITUDE\"][j].values]* nlev for j in range(nprof) ])\n",
    "        lon = np.array([[arr_data[\"LONGITUDE\"][j].values]* nlev for j in range(nprof) ])\n",
    "        posqc = np.array([[arr_data[\"POSITION_QC\"][j].values]* nlev for j in range(nprof) ])\n",
    "        juld = np.array([[arr_data[\"JULD\"][j].values]* nlev for j in range(nprof) ])\n",
    "        \n",
    "        if(i == 0):    \n",
    "            df = {'PLATFORM_NUMBER': pd.Series([str(arr_data[\"PLATFORM_NUMBER\"][0].values)]*len(ind), index=ind), \n",
    "                  'PROFILE_NUMBER' : pd.Series(NPROF.flatten(), index=ind ),\n",
    "                  'TEMP_ADJUSTED': pd.Series(arr_data[\"TEMP_ADJUSTED\"].values.flatten(), index=ind), \n",
    "                  'PSAL_ADJUSTED': pd.Series(arr_data[\"PSAL_ADJUSTED\"].values.flatten(), index=ind), \n",
    "                  'PRES_ADJUSTED': pd.Series(arr_data[\"PRES_ADJUSTED\"].values.flatten(), index=ind),  \n",
    "                  'PRES_ADJUSTED_QC': pd.Series(arr_data[\"PRES_ADJUSTED_QC\"].values.flatten(), index=ind), \n",
    "                  'PRES_ADJUSTED_ERROR':pd.Series(arr_data[\"PRES_ADJUSTED_ERROR\"].values.flatten(), index=ind), \n",
    "                  'TEMP_ADJUSTED_QC': pd.Series(arr_data[\"TEMP_ADJUSTED_QC\"].values.flatten(), index=ind), \n",
    "                  'TEMP_ADJUSTED_ERROR': pd.Series(arr_data[\"TEMP_ADJUSTED_ERROR\"].values.flatten(), index=ind), \n",
    "                  'PSAL_ADJUSTED_QC': pd.Series(arr_data[\"PSAL_ADJUSTED_QC\"].values.flatten(), index=ind),  \n",
    "                  'PSAL_ADJUSTED_ERROR':pd.Series(arr_data[\"PSAL_ADJUSTED_ERROR\"].values.flatten(), index=ind),  \n",
    "                  'JULD': pd.Series(juld.flatten(), index=ind),  \n",
    "                  'LATITUDE': pd.Series(lat.flatten(), index=ind),  \n",
    "                  'LONGITUDE': pd.Series(lon.flatten(), index=ind),  \n",
    "                  'POSITION_QC': pd.Series(posqc.flatten(), index=ind) }\n",
    "            df = pd.DataFrame(df)\n",
    "        else:\n",
    "            df_i = {'PLATFORM_NUMBER': pd.Series([str(arr_data[\"PLATFORM_NUMBER\"][0].values)]*len(ind), index=ind), \n",
    "                  'PROFILE_NUMBER' : pd.Series(NPROF.flatten(), index=ind ),\n",
    "                  'TEMP_ADJUSTED': pd.Series(arr_data[\"TEMP_ADJUSTED\"].values.flatten(), index=ind), \n",
    "                  'PSAL_ADJUSTED': pd.Series(arr_data[\"PSAL_ADJUSTED\"].values.flatten(), index=ind), \n",
    "                  'PRES_ADJUSTED': pd.Series(arr_data[\"PRES_ADJUSTED\"].values.flatten(), index=ind),  \n",
    "                  'PRES_ADJUSTED_QC': pd.Series(arr_data[\"PRES_ADJUSTED_QC\"].values.flatten(), index=ind), \n",
    "                  'PRES_ADJUSTED_ERROR':pd.Series(arr_data[\"PRES_ADJUSTED_ERROR\"].values.flatten(), index=ind), \n",
    "                  'TEMP_ADJUSTED_QC': pd.Series(arr_data[\"TEMP_ADJUSTED_QC\"].values.flatten(), index=ind), \n",
    "                  'TEMP_ADJUSTED_ERROR': pd.Series(arr_data[\"TEMP_ADJUSTED_ERROR\"].values.flatten(), index=ind), \n",
    "                  'PSAL_ADJUSTED_QC': pd.Series(arr_data[\"PSAL_ADJUSTED_QC\"].values.flatten(), index=ind),  \n",
    "                  'PSAL_ADJUSTED_ERROR':pd.Series(arr_data[\"PSAL_ADJUSTED_ERROR\"].values.flatten(), index=ind),  \n",
    "                  'JULD': pd.Series(juld.flatten(), index=ind),  \n",
    "                  'LATITUDE': pd.Series(lat.flatten(), index=ind),  \n",
    "                  'LONGITUDE': pd.Series(lon.flatten(), index=ind),  \n",
    "                  'POSITION_QC': pd.Series(posqc.flatten(), index=ind)  }\n",
    "            df_i = pd.DataFrame(df_i)\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            df = df.append(df_i, ignore_index=True)\n",
    "            del(arr_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm = create_merged_df(filenamesMerged[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7948309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=5586000, step=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm['TEMP_ADJUSTED_QC'] = dfm['TEMP_ADJUSTED_QC'].str.decode(\"utf-8\")\n",
    "dfm['PRES_ADJUSTED_QC'] = dfm['PRES_ADJUSTED_QC'].str.decode(\"utf-8\")\n",
    "dfm['PSAL_ADJUSTED_QC'] = dfm['PSAL_ADJUSTED_QC'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm[\"JULD\"] = pd.to_datetime(dfm['JULD'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm[\"DEPTH\"] = gsw.z_from_p(dfm[\"PRES_ADJUSTED\"], dfm[\"LATITUDE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SA = gsw.SA_from_SP(dfm['PSAL_ADJUSTED'],dfm['PRES_ADJUSTED'],dfm['LONGITUDE'],dfm['LATITUDE'])\n",
    "CT = gsw.CT_from_t(SA, dfm['TEMP_ADJUSTED'], dfm['PRES_ADJUSTED'])\n",
    "dfm['DENSITY_INSITU'] = gsw.density.rho(SA, CT, dfm['PRES_ADJUSTED'])\n",
    "dfm['POT_DENSITY'] = gsw.density.sigma0(SA, CT)\n",
    "dfm['CTEMP'] = CT\n",
    "dfm['SA'] = SA\n",
    "del(SA, CT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_notbad_temp = ~(dfm['TEMP_ADJUSTED_QC'].str.contains('4'))\n",
    "mask_notbad_sal = ~(dfm['PSAL_ADJUSTED_QC'].str.contains('4'))\n",
    "mask_notbad_pres = ~(dfm['PRES_ADJUSTED_QC'].str.contains('4'))\n",
    "mask_below60S = (dfm['LATITUDE'] < -60)\n",
    "mask_all = (mask_notbad_pres & mask_notbad_sal & mask_notbad_temp & mask_below60S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmg = dfm[mask_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/km/anaconda2/envs/py35new/lib/python3.5/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dfmg['DIST_GLINE'] = np.empty(len(dfmg))\n",
    "\n",
    "grndLine = gpd.read_file(\"/media/data/Datasets/Shapefiles/AntarcticGroundingLine/GSHHS_f_L6.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following distance computation is a very compute intensive and memory intensive process, ensure adequate RAM (> 16GB)\n",
    "\n",
    "### Else, use the dfmg.csv file that has already been generated using the scripts in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_distance_to_groundingline(gdf, grndLine):\n",
    "    indices = gdf.index\n",
    "    df_lonlat = np.array([[gdf.loc[indices[0], 'LONGITUDE'],  gdf.loc[indices[0], 'LATITUDE'] ]])\n",
    "    point = Point(df_lonlat[0])\n",
    "    closest_polygon_ind = np.argmin(grndLine.geometry[:].distance(point))\n",
    "\n",
    "    poly_points = np.array(list(grndLine.geometry[closest_polygon_ind].exterior.coords))\n",
    "    closest_gl_point_ind = np.argmin(cdist(poly_points , df_lonlat))\n",
    "    \n",
    "    distance = float(haversine(poly_points[closest_gl_point_ind, ::-1], df_lonlat[0, ::-1]))\n",
    "    #print(distance, type(distance))\n",
    "    gdf.loc[:, 'DIST_GLINE'] = distance\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmg = dfmg.groupby(['LATITUDE', 'LONGITUDE']).apply(find_distance_to_groundingline, grndLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following only if you want a column with ECHODEPTH created\n",
    "Every lon lat point in the dataframe will be assigned an echodepth based on the closest available point in the GEBCO 2014 bathymetry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmg = pd.read_csv(\"dfmg.csv\")\n",
    "del dfmg['Unnamed: 0']\n",
    "dfmg.loc[:,'JULD'] = pd.to_datetime(dfmg.loc[:, 'JULD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bathy = netcdf.netcdf_file('/media/data/Datasets/Bathymetry/GEBCO_2014_2D.nc', 'r') ## give path to bathymetry file\n",
    "lons = bathy.variables['lon'][:]\n",
    "lats = bathy.variables['lat'][:]\n",
    "dfmg['ECHODEPTH'] = np.nan\n",
    "\n",
    "def assign_echodepth(df):\n",
    "    ind = df.index\n",
    "    lon = df.loc[ind[0], 'LONGITUDE']\n",
    "    lat = df.loc[ind[0],'LATITUDE']\n",
    "    ind_lon = np.argmin(np.abs(lons - lon))\n",
    "    ind_lat = np.argmin(np.abs(lats - lat))\n",
    "    df.loc[:,'ECHODEPTH'] = bathy.variables['elevation'][ind_lat, ind_lon]\n",
    "\n",
    "    return df\n",
    "    \n",
    "dfmg = dfmg.groupby(['LATITUDE', 'LONGITUDE']).apply(assign_echodepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plays a beep sound. Useful to notify once any script is done running\n",
    "def play_beep():\n",
    "    import os\n",
    "    duration = 1  # second\n",
    "    freq = 440  # Hz\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))\n",
    "play_beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3353511"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmg.loc[:, \n",
    "         [\"PROFILE_NUMBER\", \"PSAL_ADJUSTED\", \"TEMP_ADJUSTED\", \"PRES_ADJUSTED\", \"LONGITUDE\", \"LATITUDE\"]]\\\n",
    "          .to_csv(\"STPlonlat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Densities ($\\gamma_N$) is produced using the matlab script: \"compute_gamman.m\"\n",
    "\n",
    "The matlab script produces gamman.csv which should then be integrated into the dfmg.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmg.to_csv(\"dfmg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3353511"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199169"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfmg.PROFILE_NUMBER.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
